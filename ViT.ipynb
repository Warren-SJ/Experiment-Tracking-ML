{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from scripts import data_setup, data_acquisition, engine, predict, plot_loss_curves\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from random import sample\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi_20%\"\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path /\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup manual transforms as required in the ViT\n",
    "\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize(size = (224,224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell can be used to download a dataset from a URL and extract it to a specified directory\n",
    "\n",
    "# This is a smaller version of the Food101 dataset\n",
    "\n",
    "# data_acquisition.acquire_data(image_path=image_path,\n",
    "#                               url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "#                               zip_name = \"pizza_steak_sushi_20%\")\n",
    "\n",
    "# train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(data_dir=image_path,\n",
    "#                                                                                train_transform=manual_transforms,\n",
    "#                                                                                test_transform=manual_transforms,\n",
    "#                                                                                batch_size=BATCH_SIZE,\n",
    "#                                                                                num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain required dataset. This may be commented out and the cell may be run again to download the dataset from a URL\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = data_path,\n",
    "                                  train = True,\n",
    "                                  transform = manual_transforms,\n",
    "                                  download = True)\n",
    "test_dataset = datasets.CIFAR10(root = data_path,\n",
    "                                train = False,\n",
    "                                transform = manual_transforms,\n",
    "                                download = True)\n",
    "\n",
    "# Setup the dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=NUM_WORKERS)\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect our data\n",
    "\n",
    "print(f\"Number of train images: {len(train_dataset)}, Number of test images: {len(test_dataset)}\\n\")\n",
    "print(f\"Number of train batches: {len(train_dataloader)}, Number of test batches: {len(test_dataloader)}\\n\")\n",
    "print(f\"{len(train_dataset) // BATCH_SIZE} batches in the train dataloader expected and {len(test_dataset) // BATCH_SIZE} batches in the test dataloader expected.\\nNote: This may not be exactly equal and may differ by 1 batch due to rounding off\\n\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a single image\n",
    "first_image_batch, first_label_batch = next(iter(train_dataloader))\n",
    "random_image_index = torch.randint(low = 0, high = len(first_image_batch), size=(1,)).item()\n",
    "\n",
    "random_image = first_image_batch[random_image_index]\n",
    "random_label = first_label_batch[random_image_index].item()\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.imshow(random_image.permute(1,2,0))\n",
    "plt.title(class_names[random_label])\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{z}_0=[\\mathbf{x}_{\\mathrm{class}}; \\mathbf{x}_p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E};\\cdots; \\mathbf{x}_p^N\\mathbf{E}]+\\mathbf{E}_{pos}\n",
    "\\end{align}\n",
    "\n",
    "is the first equation. Here, we compute the input to the transformer encoder. \n",
    "\\begin{align}\n",
    "\\mathbf{x}_{\\mathrm{class}} \n",
    "\\end{align}\n",
    "is a learnable class token\n",
    "\\begin{align}\n",
    "\\mathbf{x}_p^i\n",
    "\\end{align}\n",
    "is the i-th patch embedding obtained from the linear projection of the flattened image\n",
    "\\begin{align}\n",
    "\\mathbf{E}_{pos}^i\n",
    "\\end{align}\n",
    "is the positional embedding corresponding to the i-th patch position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second euqation describes the multi-head self attention (MSA). MSA mechanism applies self-attention multiple times in parallel.\n",
    "\n",
    "Attention is when a token focues on another token and an attention head is a single instance of the self-attention mechanism.\n",
    "\n",
    "First, 3 learned projection matrices are used to compute the matrices for Queires(Q), Keys(K) and Values(V), where each matrix is of an embedding dimension d Then, attention is calculated as\n",
    "\\begin{align}\n",
    "Attention(Q,K,V) = softmax(\\dfrac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{align}\n",
    "\n",
    "Softmax is used to make sure all attention scores are positive and sum up to 1. Attention scores are used to compute a weighted sum of the values V.\n",
    "\n",
    "In MSA, we apply self-attention multiple times in parallel using different learned projection matrices for each head. This allows the model to capture different aspects of the relationship between tokens. Then, the attention heads are concatenated.\n",
    "\n",
    "The second equation is,\n",
    "\\begin{align}\n",
    "\\mathbf{z^{\\prime}}_\\ell=\\mathrm{MSA}(\\mathrm{LN}(\\mathbf{z}_{\\ell-1}))+\\mathbf{z}_{\\ell-1}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third equation describes the MLP part of the ViT. It is a feedforward neural network and it operates on each token independantly.\n",
    "\\begin{align}\n",
    "z_{l+1} = MLP(LN(z^{\\prime}_l)) + z^{\\prime}_l\n",
    "\\end{align}\n",
    "\n",
    "The activatoin function used here is GELU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final equation is the normalization of the previous layer. This summarizes the previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication\n",
    "\n",
    "First we take a look at patch embeddings. If data can be represented in a learnable way, it is much easier to train a model. Here we use patches of size 16\\*16. The number of patches depends on the original size (which is 224\\*244\\*3 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "color_channels = 3\n",
    "patch_size = 16\n",
    "\n",
    "number_of_patches = (height * width) // (patch_size ** 2)\n",
    "print(f\"Number of patches for image of size {height}x{width} with patch size {patch_size}: {number_of_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "embegging_layer_output_shape = (number_of_patches, patch_size **2 * color_channels)\n",
    "\n",
    "print(f\"Input shape to embedding layer: {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape from embedding layer: {embegging_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a single image into patches\n",
    "\n",
    "In this section, we take the previous random image and convert it to patches, and in this example, a 224\\*224 image to many images of size 16\\*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create an array of zeros with the shape of the number of patches, patch size, patch size, and color channels.\n",
    "# The patch size, patch size represents the height and width of the patch and the color channels represent the number of channels in the image (3 for RGB).\n",
    "random_image_permuted = random_image.permute(1,2,0) # We permute the image to have the color channels at the end\n",
    "rows = np.zeros ((height // patch_size, patch_size, width, color_channels))\n",
    "for i in range(height // patch_size):\n",
    "    row = random_image_permuted[i * patch_size: (i + 1) * patch_size, :,:]\n",
    "    rows[i] = row\n",
    "print(f\"Shape of rows: {rows.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = height // patch_size, \n",
    "                         ncols = 1, \n",
    "                         figsize = (12, 12))\n",
    "fig.suptitle(f\"{class_names[random_label]} broken down into rows\")\n",
    "for i,row in enumerate(rows):\n",
    "    axes[i].imshow(row)\n",
    "    axes[i].axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have seperated the rows into rows of height 1. Now, each row has to be seperated into patches of size 16\\*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = np.zeros((number_of_patches, patch_size, patch_size, color_channels))\n",
    "for i,row in enumerate(rows):\n",
    "    for j in range(width // patch_size):\n",
    "        patch = row[:, j * patch_size: (j + 1) * patch_size,:]\n",
    "        patches[i * (width // patch_size) + j] = patch\n",
    "print(patches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = height // patch_size, \n",
    "                         ncols = width // patch_size, \n",
    "                         figsize = (12, 12))\n",
    "fig.suptitle(f\"{class_names[random_label]} broken down into patches\")\n",
    "for i in range(height // patch_size):\n",
    "    for j in range(width // patch_size):\n",
    "        axes[i,j].imshow(patches[i * width // patch_size + j])\n",
    "        axes[i,j].axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating patches with torch.nn.Conv2d\n",
    "\n",
    "According to the ViT paper, \n",
    "\n",
    "`As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1Ã—1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.`\n",
    "\n",
    "Therefore, instead of just breaking the image into 16\\*16 blocks, we can use a 2D convolutional layer to split the image and create a learnable embedding.\n",
    "\n",
    "To create patches of required size, we set the kernel size to P and the stride also to P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_channels = 3, # 3 for RGB\n",
    "                   out_channels = 768, # 768 is the number of hidden units in the ViT base model\n",
    "                   kernel_size = patch_size, \n",
    "                   stride = patch_size,\n",
    "                   padding = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_after_convolution = conv2d(random_image.unsqueeze(0)) # We add a batch dimension\n",
    "print(f\"Shape of image after convolution: {image_after_convolution.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = sample(population=range(768),\n",
    "                        k = 5)\n",
    "print(f\"Random indices = {random_indices}\")\n",
    "fig, axes = plt.subplots(nrows = 1,\n",
    "                         ncols = 5,\n",
    "                         figsize = (20, 12))\n",
    "squeezed_image = image_after_convolution.squeeze(dim = 0)\n",
    "for i, index in enumerate(random_indices):\n",
    "    axes[i].imshow(squeezed_image[index].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "\n",
    "Next, we need to flatten the learned embeddings. This needs to be done outside the number of out shannels dimensions. That is, we still want to make sure that the outermost shape is 768 as in the case of ViT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenLayer = nn.Flatten(start_dim = 2, \n",
    "                          end_dim = 3)\n",
    "# Only the 14x14 patches are flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_patches = flattenLayer(image_after_convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of flattened patches: {flattened_patches.shape}\")\n",
    "# We have successfully flattened the patches while maintaining the batch size and the number of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = sample(population=range(768),\n",
    "                        k = 5)\n",
    "print(f\"Random indices = {random_indices}\")\n",
    "fig, axes = plt.subplots(nrows = 5,\n",
    "                         ncols = 1,\n",
    "                         figsize = (20, 12))\n",
    "for i, index in enumerate(random_indices):\n",
    "    patch_plot = flattened_patches[0][index].unsqueeze(dim = 0) # 0 is used to index into the first 'batch'\n",
    "    axes[i].imshow(patch_plot.detach().numpy())\n",
    "    axes[i].axis(False)\n",
    "    axes[i].set_title(f\"Patch {index}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for flattening is because, originally, the transformer was designed to work with text and hence by transforming, we are essentially converting our 2D image into a 1D text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a class for creating patch embeddings\n",
    "\n",
    "In this section, we create a class out of the code above. We let it inherit from nn.Module so that we can easily use it in a pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Creates a patch embedding layer. This takes in a 2D image and \n",
    "    converts it into a sequence of learnable patches.\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        patch_size (int): The height and width of each patch.\n",
    "        emb_dim (int): The embedding dimension of the output patches.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels = 3,\n",
    "                 patch_size = 16,\n",
    "                 out_channels = 768):\n",
    "        super().__init__()\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size)\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=2, \n",
    "                                  end_dim=3)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_patched  = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        return x_flattened.permute(0,2,1) # This is because the encoder expects (batch_size, num_patches, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchify = PatchEmbedding(in_channels = 3,\n",
    "                            patch_size = 16,\n",
    "                            out_channels = 768)\n",
    "patchified_image = patchify(random_image.unsqueeze(0)) # We need to pass in the image with a batch dimension\n",
    "\n",
    "print(f\"Shape of patchified image: {patchified_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upto this point, we have coded the patch embedding part of the vision transformer in the first equation. What's left is the position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model = patchify,\n",
    "        input_size = (32,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\",\"trainable\"],\n",
    "        row_settings = [\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = patchified_image.shape[0]\n",
    "embedding_dimension = patchified_image.shape[2]\n",
    "\n",
    "class_token = nn.Parameter(torch.randn(batch_size, 1, embedding_dimension), requires_grad=True)\n",
    "# (batch_size -> for each image im the batch, 1-> 1 class token, embedding_dimension->for each embedding)\n",
    "\n",
    "print(f\"Shape of class token: {class_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchified_image_with_class = torch.cat((class_token, patchified_image), dim = 1)\n",
    "print(f\"Shape of patchified image with class token: {patchified_image_with_class.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding = nn.Parameter(torch.randn(1, number_of_patches + 1, embedding_dimension), requires_grad=True)\n",
    "print(f\"Shape of position embedding: {position_embedding.shape}\")\n",
    "position_and_patch_embedding  = patchified_image_with_class + position_embedding\n",
    "print(f\"Shape of position and patch embedding: {position_and_patch_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief summary: The input to the encoder contains:\n",
    " * patch embedding: This is a learnable embedding of size(batch_size, number_of_patches,embedding_dimension)\n",
    " * class token: This is a learnable class token of size(batch_size,1,embedding_dimension)\n",
    " * position embedding: This is a learnable embedding of size(batch_size, number_of_patches + 1, embedding_dimension)\n",
    "\n",
    "First, the class token is concatenated with the patch embedding to produce a tensor of size (batch_size, number_of_patches + 1,embedding_dimension). The it's shape matches that of teh position embedding and can be added together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating all into a single code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "\n",
    "print(f\"Random image shape: {random_image.shape}\")\n",
    "\n",
    "random_image_with_batch = random_image.unsqueeze(0)\n",
    "print(f\"Random image shape with batch: {random_image_with_batch.shape}\")\n",
    "\n",
    "patchify = PatchEmbedding(in_channels = 3,\n",
    "                          patch_size = patch_size,\n",
    "                          out_channels=768)\n",
    "\n",
    "patch_embedded_image  = patchify(random_image_with_batch)\n",
    "print(f\"Shape of patch embedded image: {patch_embedded_image.shape}\")\n",
    "\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "number_of_patches = patch_embedded_image.shape[1]\n",
    "embedding_dimension = patch_embedded_image.shape[2]\n",
    "\n",
    "class_token = nn.Parameter(torch.randn(batch_size, 1, embedding_dimension), requires_grad=True)\n",
    "print(f\"Shape of class token: {class_token.shape}\")\n",
    "\n",
    "patch_embedded_image_with_class = torch.cat((class_token, patch_embedded_image), dim = 1)\n",
    "print(f\"Shape of patch embedded image with class token: {patch_embedded_image_with_class.shape}\")\n",
    "\n",
    "position_embedding = nn.Parameter(torch.randn(1, number_of_patches + 1, embedding_dimension), requires_grad=True)\n",
    "print(f\"Shape of position embedding: {position_embedding.shape}\")\n",
    "\n",
    "position_and_patch_embedding = patch_embedded_image_with_class + position_embedding\n",
    "print(f\"Shape of position and patch embedding: {position_and_patch_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the MSA Block\n",
    "\n",
    "In this section, we create the MSA block which is the main focus in equation 2 of the transformer equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a single MultiHeadSelfAttentionBlock.\n",
    "    Args:\n",
    "        embedding_dimension (int): The embedding dimension of the input.\n",
    "        num_heads (int): The number of heads in the multi-head attention block.\n",
    "        attention_dropout (float): The dropout rate to apply to the attention scores.\n",
    "        \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dimension :int = 768, # ViT base has 768 embedding dimensions so we set 768 by default\n",
    "                 num_heads : int = 12, # ViT base has 12 heads so we set 12 by default\n",
    "                 attention_dropout : float = 0): # No dropout by default for ViT\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dimension)\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embed_dim=embedding_dimension,\n",
    "                                                          num_heads=num_heads,\n",
    "                                                          dropout=attention_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output,_ = self.multi_head_attention(query = x,\n",
    "                                                  key = x,\n",
    "                                                  value = x,\n",
    "                                                  need_weights=False)\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_block = MultiHeadSelfAttentionBlock(embedding_dimension=768,\n",
    "                                        num_heads=12,\n",
    "                                        attention_dropout=0)\n",
    "\n",
    "msa_output = msa_block(position_and_patch_embedding)\n",
    "print(f\"Shape of MultiHeadSelfAttentionBlock output: {msa_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation 3: The MLP\n",
    "\n",
    "In ViT, the MLP consists of 2 layers with GELU activation. In this section, we replicate this with layers and create a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a single MLP block with following structure:\n",
    "    linear->GELU->dropout->linear->dropout\n",
    "    Args:\n",
    "    embedding_dimension (int): The embedding dimension of the input.\n",
    "    mlp_size (int): size of each layer of MLP.\n",
    "    mlp_dropout (float): dropout probability to be used in the dropout layers\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dimension:int = 768,\n",
    "                 mlp_size: int = 3072,\n",
    "                 mlp_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dimension)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_dimension,\n",
    "                      out_features = mlp_size,\n",
    "                      bias= True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = mlp_dropout),\n",
    "            nn.Linear(in_features= mlp_size,\n",
    "                      out_features= embedding_dimension,\n",
    "                      bias = True),\n",
    "            nn.Dropout(p = mlp_dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpblock = MLPBlock(embedding_dimension = 768,\n",
    "                    mlp_size = 3072,\n",
    "                    mlp_dropout = 0.1)\n",
    "\n",
    "patched_image_after_mlp_block = mlpblock(msa_output)\n",
    "print(f\"Shape after MLP block: {patched_image_after_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the transformer encoder\n",
    "\n",
    "An encoder is a stack of layers that converts an input into some form of numerical representation. According to the ViT paper,\n",
    "\n",
    "`The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).`\n",
    "\n",
    "MSA and MLP layers have been implemented above. What's left are the residual connections. These connections add back the input to the output of the MSA and MLP blocks.\n",
    "\n",
    "Why add skip connections? Skip connections improve accuracy and generalization and also solve the vanishing gradient probelm.\n",
    "\n",
    "In order to create the transformer encoder, we create a class of it and use the above created MSA and MLP blocks. The input to the transformer encoder are the patch and position embeddings along with the class token we created as part of equation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Creates a transformer encoder block. It creates an MSA and MLP block\n",
    "    Args:\n",
    "    embedding_dimension (int): The embedding dimension of the input.\n",
    "    num_heads (int): The number of heads in the multi-head attention block.\n",
    "    attention_dropout (float): The dropout rate to apply to the attention scores.\n",
    "    mlp_size (int): size of each layer of MLP.\n",
    "    mlp_dropout (float): dropout probability to be used in the dropout layers\n",
    "    \"\"\"\n",
    "    def __init__(self, # All default values are what are used in ViT base model\n",
    "                 embedding_dimension: int = 768, \n",
    "                 num_heads: int = 12,\n",
    "                 attention_dropout: float = 0.0,\n",
    "                 mlp_size: int = 3072,\n",
    "                 mlp_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.msablock = MultiHeadSelfAttentionBlock(embedding_dimension = embedding_dimension,\n",
    "                                                    num_heads = num_heads,\n",
    "                                                    attention_dropout = attention_dropout)\n",
    "        self.mlpblock = MLPBlock(embedding_dimension = embedding_dimension,\n",
    "                                 mlp_size = mlp_size,\n",
    "                                 mlp_dropout = mlp_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.msablock(x) + x\n",
    "        x = self.mlpblock(x) + x\n",
    "        # Above, weadd residual connections as stated in the ViT paper\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformerencoder = TransformerEncoder(embedding_dimension = 768,\n",
    "                                        num_heads = 12,\n",
    "                                        attention_dropout = 0.0,\n",
    "                                        mlp_size = 3072,\n",
    "                                        mlp_dropout = 0.1)\n",
    "\n",
    "print(summary(model = transformerencoder, \n",
    "              input_size = (1,197,768), \n",
    "              col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate method is to use pytorch's built in torch.nn.TransformerEncoderLayer(). This makes it easier to create a transformer encoder and also it may be less prone to errors and may even be faster in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transformer_encoder = nn.TransformerEncoderLayer(d_model = 768,\n",
    "                                                       nhead = 12,\n",
    "                                                       dim_feedforward = 3072,\n",
    "                                                       dropout = 0.1,\n",
    "                                                       activation = \"gelu\",\n",
    "                                                       batch_first = True,\n",
    "                                                       norm_first = True)\n",
    "print(torch_transformer_encoder)\n",
    "print(summary(model = torch_transformer_encoder,\n",
    "              input_size = (1,197,768), \n",
    "              col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "              row_settings=[\"var_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transformer_encoder_layers = nn.TransformerEncoder(encoder_layer=torch_transformer_encoder,\n",
    "                                                         num_layers = 12)\n",
    "print(torch_transformer_encoder_layers)\n",
    "print(summary(model = torch_transformer_encoder_layers,\n",
    "              input_size = (1,197,768), \n",
    "              col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "              row_settings=[\"var_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "In this section, we combine all the previous steps and create the ViT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters\n",
    "    Args:\n",
    "        img_size (int): Size of the input images (assumed to be square).\n",
    "        in_channels (int): Number of input channels.\n",
    "        patch_size (int): Size of the patches to split the image into.\n",
    "        embedding_dimension (int): Dimensionality of the patch embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        mlp_size (int): Hidden layer size in the MLP block.\n",
    "        attn_dropout (float): Dropout rate for the attention layer.\n",
    "        mlp_dropout (float): Dropout rate for the MLP block.\n",
    "        embedding_dropout (float): Dropout rate for the patch embeddings.\n",
    "        num_transformer_encoders (int): Number of transformer encoder layers.\n",
    "        num_classes (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size:int = 224,\n",
    "                 in_channels:int = 3,\n",
    "                 patch_size:int = 16,\n",
    "                 embedding_dimension:int = 768,\n",
    "                 num_heads:int = 12,\n",
    "                 mlp_size:int = 3072,\n",
    "                 attn_dropout:float = 0,\n",
    "                 mlp_dropout:float = 0.1,\n",
    "                 embedding_dropout:float = 0.1,\n",
    "                 num_transformer_encoders:int = 12,\n",
    "                 num_classes:int = 10\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Make sure image size is divisible by patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size {img_size} not divisible by patch size {patch_size} \"\n",
    "\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        # Create the patch embedding. This applies for every image in the batch\n",
    "        self.patch_embedding = PatchEmbedding(in_channels = in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              out_channels = embedding_dimension)\n",
    "\n",
    "        # Class token is for 1 image in the batch\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dimension), requires_grad=True)\n",
    "\n",
    "        # Similarly, position embedding is for 1 image in the batch\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dimension), requires_grad=True)\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n",
    "\n",
    "        self.transformer_encoders = nn.Sequential(*[TransformerEncoder(embedding_dimension = embedding_dimension,\n",
    "                                                                       num_heads = num_heads,\n",
    "                                                                       attention_dropout = attn_dropout,\n",
    "                                                                       mlp_size = mlp_size,\n",
    "                                                                       mlp_dropout = mlp_dropout) for _ in range(num_transformer_encoders)])\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape = embedding_dimension),\n",
    "                                        nn.Linear(in_features = embedding_dimension,\n",
    "                                                  out_features = num_classes))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim = 1)\n",
    "\n",
    "        position_embedding = self.position_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x += position_embedding\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoders(x)\n",
    "\n",
    "        x = self.classifier(x[:,0])\n",
    "\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating the class embedding and expanding over a batch dimension\n",
    "batch_size = 32\n",
    "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
    "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
    "\n",
    "# Print out the change in shapes\n",
    "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
    "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_image_tensor = torch.randn(16, 3, 224, 224).cuda() # (batch_size, color_channels, height, width)\n",
    "\n",
    "# # Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "# vit = ViT(num_classes=len(class_names)).to(device)\n",
    "\n",
    "# # Pass the random image tensor to our ViT instance\n",
    "# vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
    "# summary(model=vit,\n",
    "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "#                              weight_decay=0.3,\n",
    "#                              betas=(0.9,0.999),\n",
    "#                              lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history, all_preds, all_labels, writer = engine.train(model = vit,\n",
    "#                                                     model_name=\"ViT\",\n",
    "#                                                     data_name=\"CIFAR10\",\n",
    "#                                                     epochs = 5,\n",
    "#                                                     train_dataloader=train_dataloader,\n",
    "#                                                     val_dataloader=test_dataloader,\n",
    "#                                                     device=device,\n",
    "#                                                     optimizer=optimizer,\n",
    "#                                                     loss_fn=loss_fn,    \n",
    "#                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing Transfer Learning\n",
    "\n",
    "In this section, we will use a pre existing ViT model and fine tune it on the CIFAR-10 dataset. We will use the ViT model from the torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "\n",
    "pre_trained_vit = torchvision.models.vit_b_16(weights = weights).to(device)\n",
    "\n",
    "summary(model = pre_trained_vit,\n",
    "        input_size = (32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in pre_trained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "summary(model = pre_trained_vit,\n",
    "        input_size = (32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_vit.heads = nn.Linear(in_features = 768,\n",
    "                                    out_features = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model = pre_trained_vit,\n",
    "        input_size = (32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=pre_trained_vit.parameters(), \n",
    "                             weight_decay=0.3,\n",
    "                             betas=(0.9,0.999),\n",
    "                             lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, all_preds, all_labels, writer = engine.train(model = pre_trained_vit,\n",
    "                                                        model_name=\"ViT\",\n",
    "                                                        data_name=\"CIFAR10\",\n",
    "                                                        epochs = 5,\n",
    "                                                        train_dataloader=train_dataloader,\n",
    "                                                        val_dataloader=test_dataloader,\n",
    "                                                        device=device,\n",
    "                                                        optimizer=optimizer,\n",
    "                                                        loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves.plot_loss_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
